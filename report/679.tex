\documentclass[11pt]{article} % use larger type; default would be 10pt
\usepackage[utf8]{inputenc}
%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{letterpaper} % or letterpaper (US) or a5paper or....
\geometry{left=2 cm,right=2 cm,top=2.5 cm,bottom=2.5 cm}
\usepackage{graphicx} % support the \includegraphics command and options

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another...
\usepackage{amsmath}

\begin{document}
\section{Asymptotic Properties}
The author derives asymptotic properties of both lasso-type and garrote-type estimators to provide illustrations of mechanism of the estimators. We only consider the assumption that $p$ is held as $n\rightarrow \infty$, while $p\rightarrow \infty$ and $n\rightarrow \infty$ should be more realistic. 

\subsection{Lasso-Type Estimator}
\subsubsection{Theorem (Theorem 1)}
If $\sqrt{n} \lambda \rightarrow \lambda_0 \leq 0$ as $n\rightarrow \infty$, the lasso-type estimator is such that 
\[\sqrt{n} \left( \hat{C} - C\right) \stackrel{d}{\rightarrow} \mbox{arg} \min_U \left( V\right) \mbox{,} \] 
where
\[V(U)=tr(U\Sigma U\Sigma )+tr(UW)+\lambda_0 \sum_{i\neq j}\left\lbrace u_{ij} \mbox{sign} (c_{ij})I(c_{ij}\neq 0)+|u_{ij}| I(c_{ij} =0) \right\rbrace \mbox{,}\]
in which $W$ is a random symmetric $p\times p$ matrix such that $vec(W)\sim N(0, \Lambda)$, and $\Lambda$ is such that $cov(w_{ij},w_{i'j'})=cov(X^{(i)}X^{(j)},X^{(i')}X^{(j')})$. \\

The theorem shows that the lasso-type estimator converges to a limiting distribution as $n\rightarrow \infty$ with a proper choice of $\lambda$. It also reveals the weakness of lasso-type estimator, for it has no guarantee of sparsity and consistency of model selection, and it is asymptotically biased rather than a consistent estimator of the concentration matrix.

\subsubsection{Summary of Proof}
\begin{itemize}
\item First, let the estimator $\hat{C}=C+\frac{U}{\sqrt{n}}$. Subtly define $V_n(U)$ as the difference between the objective function values of $\hat{C}$ and $C$: \[V_n(U)=-\log\left\vert C+\frac{U}{\sqrt{n}}\right\vert +tr\left\lbrace \left( C+\frac{U}{\sqrt{n}}\right) \bar{A}\right\rbrace +\lambda \sum_{i\neq j} \left\vert c_{ij}+\frac{u_{ij}}{\sqrt{n}}\right\vert +\log|C|-tr(C\bar{A})-\lambda \sum_{i\neq j}|c_{ij}|\mbox{,}\]
the first part of which is the objective function and the second part is a constant of $U$. 
\item By some algebraic manipulation, $nV_n(U)$ can be re-written as \[tr(U\Sigma U\Sigma )+tr(UW_n)+\sqrt{n} \lambda \sum_{i\neq j}\left\lbrace u_{ij} \mbox{sign} (c_{ij})I(c_{ij}\neq 0)+|u_{ij}| I(c_{ij} =0)\right\rbrace +o(1)\mbox{,}\]
where $W_n=\sqrt{n}(\bar{A}-\Sigma)\rightarrow N(0,\Lambda)$. 
\item Because $W_n\rightarrow W$ and $\sqrt{n} \lambda \rightarrow \lambda_0$, we have $nV_n(U)\rightarrow V(U)$ in distribution. 
\item Since both $V(U)$ and $V_n(U)$ are convex and $V(U)$ has a unique minimum, \[\mbox{arg} \min nV_n\left( U\right)=\sqrt{n} \left( \hat{C} - C\right) \stackrel{d}{\rightarrow} \mbox{arg} \min V\left( U\right) \mbox{.} \]
\end{itemize}
\subsubsection{Illustration}
To have a more explicit view of the asymptotic distribution, give an example where $p=3$ as an illustration. 
\[C=\left( \begin{array}{ccc}
1 & \frac{1}{3} & 0 \\
\frac{1}{3} & 1 & \frac{2}{3} \\
0 & \frac{2}{3} & 1 \end{array} \right) \mbox{,}\qquad
\Sigma=\left( \begin{array}{ccc}
1.25 & -0.75 & 0.5 \\
-0.75 & 2.25 & -1.5 \\
0.5 & -1.5 & 2
\end{array} \right) \mbox{.}
\]
Simulate 1000 observations from the asymptotic distribution $\mbox{arg} \min V$, and make scatterplots of the estimated concentration matrix elements of $\lambda_0=0,\; 0.5,\; \mbox{and }1$. 
\begin{itemize}
\item For $\lambda_0=0$, the estimator is the MLE, and its asymptotic distribution is multivariate normal due to the consistency of MLE. With no penalty, it has 0 probability of estimating $c_{13}$ by its true value 0. 
\item For $\lambda_0=0.5$, there is a positive probability $Pr(\hat{c}_{13}=0)=0.3$, but the estimates of other non-zero elements are biased. 
\item For $\lambda_0=1$, the probability of $Pr(\hat{c}_{13}=0)$ increases to 0.45.
\end{itemize}

This example shows the lasso-type estimator can produce a sparse estimate of the concentration matrix. However, it produces substantial biases in the estimates for large coefficients. 

\subsection{Garrote-Type Estimator}
\subsubsection{Theorem (Theorem 2)}
With initial estimator $\tilde{C}=\bar{A}^{-1}$. If $n\lambda \rightarrow \infty$ and $\sqrt{n} \lambda \rightarrow 0$ as $n\rightarrow \infty$, then $Pr(\hat{c}_{ij}=0)\rightarrow 1$ if $c_{ij}=0$, and other elements of $\hat{C}$ have the same limiting distribution as the maximum likelihood estimator on the true graph structure. \\

It indicates that the garrote-type estimator has better asymptotic property, which is called oracle property. First, it has a consistency in model selection, which can guarantee sparsity and correct graph structure. Second, it is a consistent estimator which is asymptotically normal and unbiased. 
\subsubsection{Summary of Proof}
\begin{itemize}
\item Similarly, let $\hat{C}=C+\frac{U}{\sqrt{n}}$, and define $V_n(U)$ as the difference between the objective function values of $\hat{C}$ and $C$: 
\[V_n(U)=-\log\left\vert C+\frac{U}{\sqrt{n}}\right\vert +tr\left\lbrace \left( C+\frac{U}{\sqrt{n}}\right) \bar{A}\right\rbrace +\lambda \sum_{i\neq j} \frac{c_{ij}+u_{ij}/\sqrt{n}}{\tilde{c}_{ij}} +\log|C|-tr(C\bar{A})-\lambda \sum_{i\neq j}\frac{c_{ij}}{\tilde{c}_{ij}} \mbox{.}\]
\item As before, $V_n(U)=tr(U\Sigma U\Sigma )+tr(UW_n)+\sqrt{n} \lambda \sum_{i\neq j}\frac{u_{ij}}{\tilde{c}_{ij}} +o(1)$.
\item $\sqrt{n} \lambda \sum_{c_{ij}\neq 0}\frac{u_{ij}}{\tilde{c}_{ij}} \rightarrow 0$, because if $c_{ij}\neq 0$, $\tilde{c}_{ij} \rightarrow c_{ij}$ in probability and $\sqrt{n} \lambda \rightarrow 0$. $nV_n(U)$ can be further rewritten as $tr(U\Sigma U\Sigma )+tr(UW_n)+n\lambda \sum_{c_{ij}=0}\frac{u_{ij}}{\tilde{c}_{ij} \sqrt{n}} +o(1)$. 
\item If $c_{ij}=0$, the MLE $\tilde{c}_{ij}=o(n^{-1/2})$, then $\tilde{c}_{ij} \sqrt{n}=o(1)\rightarrow 0$. And since $n\lambda \rightarrow \infty$, the minimization of $nV_n(U)$ must satisfy $u_{ij}=0$ if $c_{ij}=0$ with probability tending to be one. 
\item We also know that $\sqrt{n} \left( \hat{C}^{MLE} - C\right) \stackrel{d}{\rightarrow} \mbox{arg} \min \left\lbrace tr(U\Sigma U\Sigma)+tr(UW)\right\rbrace$. So, the asymptotic distribution of $nV_n(U)=\sqrt{n} \left( \hat{C} - C\right)$ is the same as MLE with $u_{ij}=0$ if $c_{ij}=0$.
\end{itemize}

\section{Computation}
The computation of the proposed estimators requires optimization of nonlinear objective functions under the positive-definiteness constraint, and it can be connected with the maxdet problem. Both lasso-type and garrote-type estimators can be solved by maxdet problem. 

\subsection{Algorithm for Lasso-Type Estimator}
\subsubsection{Algorithm}
Different from the garrote-type estimator that can be directly solved by maxdet problem, lasso-type estimator can be expressed as maxdet problem only if the signs of $c_{ij}$'s are known. 
\[\min_C tr(C\bar{A})-\log|C|,\quad \mbox{subject to }C\mbox{ being positive definite, }t-2\sum_{i<j}c_{ij}s_{ij}\geq 0,\; s_{ij}c_{ij}\geq 0\]
Since the signs of $c_{ij}$'s are not known in advance, we are supposed to use the following algorithm to update $s_{ij}$'s and $c_{ij}$'s iteratively. 
\begin{itemize}
\item \emph{Step 1.} Initialize $\hat{C}_{old}=\bar{A}^{-1}$ and $s_{ij}=sign\left\lbrace (\hat{C}_{old})_{ij}\right\rbrace $ for all $i\neq j$.
\item \emph{Step 2.} Solve the maxdet problem over the set of positive definite matrices, solution $\hat{C}_{new}$. 
\item \emph{Step 3.} If $\hat{C}_{new}=\hat{C}_{old}$, then stop and let $\hat{C}=\hat{C}_{new}$. Otherwise, set $\hat{C}_{old}=\hat{C}_{new}$ and $s_{ij}=-s_{ij}$ for all pairs $(i,j)$ such that $\hat{c}_{ij}=0$ and go back to Step 2.
\end{itemize}
\subsubsection{Convergence (Lemma 3)}
The above algorithm always converges and converges to the solutions to (1). And in practice, the algorithm usually converges within a small number of iterations. \\
\emph{Summary of Proof: }
\begin{itemize}
\item The minima attained in the iterations form a strictly decreasing sequence. However, the number of possible sign matrices is finite. Therefor the algorithm has to terminate. 
\item $\hat{C}$ solves with both the two sign matrices $\hat{S}$ and $\tilde{S}$: $\hat{s}_{ij}=-\tilde{s}_{ij}$ for any $\hat{c}_{ij}=0$ and $\hat{s}_{ij}=\tilde{s}_{ij}$ for any $\hat{c}_{ij}\neq 0$, because $\hat{C}=\hat{C}_{new}=\hat{C}_{old}$.
\item By Karush-Kuhn-Tucker conditions, we have: \[\left\lbrace
\begin{array}{l}
\left.\frac{\partial l}{\partial c_{ij}}\right\vert_{C=\hat{C}}+\lambda_1 \hat{s}_{ij}-\lambda_{ij}\hat{s}_{ij}=0\\
\lambda_1\left(t-2\sum_{i<j}\hat{c}_{ij}\hat{s}_{ij}\right)=0\\
\lambda_{ij}\left(\hat{s}_{ij}\hat{c}_{ij}\right)=0
\end{array}\right.
\mbox{,}\qquad \mbox{where }\lambda_1>0\mbox{ and }\lambda_{ij}\geq 0\mbox{, }l(C)=-\log|C|+tr(C\bar{A})\mbox{.}\]
\item By some simple manipulation, \[\left\lbrace
\begin{array}{ll}
\left. \frac{\partial l}{\partial c_{ij}}\right\vert_{C=\hat{C}}\hat{s}_{ij}=-\lambda_1\mbox{, } & \mbox{for all }\hat{c}_{ij}\neq 0\\
 \left. \frac{\partial l}{\partial c_{ij}}\right\vert_{C=\hat{C}}\hat{s}_{ij}\geq -\lambda_1\mbox{, } & \mbox{for all }\hat{c}_{ij}= 0
\end{array}\right.
\mbox{. Similarly, }\left\lbrace
\begin{array}{ll}
\left. \frac{\partial l}{\partial c_{ij}}\right\vert_{C=\hat{C}}\tilde{s}_{ij}=-\lambda_2\mbox{, } & \mbox{for all }\hat{c}_{ij}\neq 0\\
 \left. \frac{\partial l}{\partial c_{ij}}\right\vert_{C=\hat{C}}\tilde{s}_{ij}\geq -\lambda_2\mbox{, } & \mbox{for all }\hat{c}_{ij}= 0
\end{array}\right.\mbox{.}
\]
\item With the fact that $\hat{s}_{ij}=\tilde{s}_{ij}$, we can conclude that $\lambda_1=\lambda_2=\lambda$. So, the algorithm converges, and also the solution is equivalently to (1). 
\end{itemize}
\subsection{Tuning}
In practice, we need to choose a tuning parameter $t$ or $\lambda$ to minimize a score measuring the goodness-of-fit. Multifold cross-validation score is commonly used, but the author recommends a computationally more efficient method using BIC. The score is defined as: 
\[BIC(t)=-\log|\hat{C}(t)|+tr\{\hat{C}(t)\bar{A}\}+\frac{\log n}{n}\sum_{i\leq j}\hat{e}_{ij}(t)\mbox{,}\]
where $\hat{e}_{ij}=0$ if $\hat{c}_{ij}=0$, and $\hat{e}_{ij}=1$ otherwise. It is negative log-likelihood with a penalty of degrees of freedom. We choose the tuning parameter $t$ which minimizes the BIC score. 



\end{document}
